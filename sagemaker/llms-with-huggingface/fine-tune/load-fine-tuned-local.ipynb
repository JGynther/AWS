{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997db4c2-3ed4-4543-b73d-cae147a0b69c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install transformers peft einops accelerate bitsandbytes torch --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cc5b84b-d281-433f-92d6-fdba0b79c154",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e78344e1b5f4b9d9a83e1e7693c35e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tiiuae/falcon-7b\")\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\"./model\", load_in_8bit=True, trust_remote_code=True, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8b09579-b27d-44db-b16c-de717a2ecc9b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'RWForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34c775a9-b479-465d-8da4-63ca9933aa5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_prompt(code: str) -> str:\n",
    "    return f\"### Input:\\n{code}\" + \"\\n\\n### Response:\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "50c878d4-0155-4621-89fe-5ac62df283a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fibonacci series - the series is `0, 1, 1, 2, 3, 5,...`\n",
      "\n",
      "@param {number} n\n",
      "@returns {void}\n",
      "@example\n",
      "fibbonaci(10)\n",
      "// prints\n",
      "// 0, 1, 1, 2, 3, 5, 8, 13, 21,...\n",
      "@example\n",
      "fibbonaci(20)\n",
      "// prints\n",
      "// 0\n"
     ]
    }
   ],
   "source": [
    "example_function = \"\"\"function fibbonaci(n) {\n",
    "    let a = 0;\n",
    "    let b = 1;\n",
    "    let next;\n",
    "\n",
    "    for (let i = 1; i<=n; ++i) {\n",
    "        console.log(a);\n",
    "        next = a + b;\n",
    "        a = b;\n",
    "        b= next;\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "prompt = format_prompt(example_function)\n",
    "\n",
    "result = pipe(\n",
    "    prompt,\n",
    "    max_length=200,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    ")\n",
    "\n",
    "print(result[0][\"generated_text\"][len(prompt):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c9fd50-ac84-476b-b1a3-2d3e3ebcf4d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
